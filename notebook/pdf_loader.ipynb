{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee530b0f",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fa367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700de23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAG-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a56c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d104fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8982f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files.\n",
      "Processing file: ..\\data\\pdf_files\\sample_machine_learning (1).pdf\n",
      "Loaded 1 pages from ..\\data\\pdf_files\\sample_machine_learning (1).pdf\n",
      "Processing file: ..\\data\\pdf_files\\sample_transformers.pdf\n",
      "Loaded 1 pages from ..\\data\\pdf_files\\sample_transformers.pdf\n",
      "Total documents loaded: 2\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file}\")\n",
    "\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source_file\"] = str(pdf_file)\n",
    "                doc.metadata[\"file_type\"]='pdf'\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages from {pdf_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {pdf_file}: {e}\")\n",
    "    \n",
    "    print(f\"Total documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee43e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-10-18T18:43:18+00:00', 'source': '..\\\\data\\\\pdf_files\\\\sample_transformers.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\sample_transformers.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251018184318Z', 'page': 0, 'source_file': '..\\\\data\\\\pdf_files\\\\sample_transformers.pdf', 'file_type': 'pdf'}, page_content='Introduction to Transformers in AI\\nTransformers are a type of deep learning model introduced in the paper \"Attention is All You Need\"\\nin 2017. \\nThey have revolutionized the field of Natural Language Processing by enabling models to handle\\nsequential data without relying on recurrent architectures.\\nThe key innovation of transformers is the self-attention mechanism, which allows the model to weigh\\nthe importance of different words in a sequence when making predictions. \\nThis capability makes transformers highly effective for tasks like language translation, text\\nsummarization, and question answering.\\nPopular transformer-based models include BERT, GPT, and T5. These models have been\\npre-trained on large corpora and can be fine-tuned for specific NLP tasks. \\nTransformers are also increasingly applied in other domains such as computer vision and speech\\nprocessing, demonstrating their versatility and power in AI applications.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b64b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db4e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2 documents into 3 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Introduction to Machine Learning\n",
      "Machine Learning (ML) is a subset of Artificial Intelligence that enables computers to learn patterns\n",
      "and make decisions without being explicitly programmed. \n",
      "Instead ...\n",
      "Metadata: {'producer': '', 'creator': '', 'creationdate': '2025-10-18T18:28:51+00:00', 'source': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251018182851Z', 'page': 0, 'source_file': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-10-18T18:28:51+00:00', 'source': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251018182851Z', 'page': 0, 'source_file': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'file_type': 'pdf'}, page_content='Introduction to Machine Learning\\nMachine Learning (ML) is a subset of Artificial Intelligence that enables computers to learn patterns\\nand make decisions without being explicitly programmed. \\nInstead of following fixed rules, ML systems analyze historical data to find trends and relationships,\\nallowing them to make predictions or classifications on new data.\\nCommon types of machine learning include supervised learning, unsupervised learning, and\\nreinforcement learning. \\nSupervised learning relies on labeled datasets to train predictive models, while unsupervised\\nlearning discovers hidden structures within unlabeled data. \\nReinforcement learning involves agents learning optimal behavior through trial and error interactions\\nwith their environment.\\nMachine learning has applications across industries, from fraud detection and recommendation\\nsystems to speech recognition and self-driving cars. \\nWith the growing availability of data and computing power, ML is becoming an essential tool for'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-10-18T18:28:51+00:00', 'source': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251018182851Z', 'page': 0, 'source_file': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf', 'file_type': 'pdf'}, page_content='systems to speech recognition and self-driving cars. \\nWith the growing availability of data and computing power, ML is becoming an essential tool for\\nsolving complex problems in both academia and industry.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2025-10-18T18:43:18+00:00', 'source': '..\\\\data\\\\pdf_files\\\\sample_transformers.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\sample_transformers.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251018184318Z', 'page': 0, 'source_file': '..\\\\data\\\\pdf_files\\\\sample_transformers.pdf', 'file_type': 'pdf'}, page_content='Introduction to Transformers in AI\\nTransformers are a type of deep learning model introduced in the paper \"Attention is All You Need\"\\nin 2017. \\nThey have revolutionized the field of Natural Language Processing by enabling models to handle\\nsequential data without relying on recurrent architectures.\\nThe key innovation of transformers is the self-attention mechanism, which allows the model to weigh\\nthe importance of different words in a sequence when making predictions. \\nThis capability makes transformers highly effective for tasks like language translation, text\\nsummarization, and question answering.\\nPopular transformer-based models include BERT, GPT, and T5. These models have been\\npre-trained on large corpora and can be fine-tuned for specific NLP tasks. \\nTransformers are also increasingly applied in other domains such as computer vision and speech\\nprocessing, demonstrating their versatility and power in AI applications.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090f578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "329caf46",
   "metadata": {},
   "source": [
    "### Text Embedding and Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2782c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a347a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 \n",
      "Model loaded successfully: 384 dimensions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1fb3c820cb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name} \")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully: {self.model.get_sentence_embedding_dimension()} dimensions\")\n",
    "        except exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91b61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1fb3e668ec0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents, embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920d9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 3 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (3, 384)\n",
      "Adding 3 documents to vector store...\n",
      "Successfully added 3 documents to vector store\n",
      "Total documents in collection: 6\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "### Generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "### store in the vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9074aa",
   "metadata": {},
   "source": [
    "### RAG RETRIEVER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499d27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d29e422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Machine Learning'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_1c95cd68_0',\n",
       "  'content': 'Introduction to Machine Learning\\nMachine Learning (ML) is a subset of Artificial Intelligence that enables computers to learn patterns\\nand make decisions without being explicitly programmed. \\nInstead of following fixed rules, ML systems analyze historical data to find trends and relationships,\\nallowing them to make predictions or classifications on new data.\\nCommon types of machine learning include supervised learning, unsupervised learning, and\\nreinforcement learning. \\nSupervised learning relies on labeled datasets to train predictive models, while unsupervised\\nlearning discovers hidden structures within unlabeled data. \\nReinforcement learning involves agents learning optimal behavior through trial and error interactions\\nwith their environment.\\nMachine learning has applications across industries, from fraud detection and recommendation\\nsystems to speech recognition and self-driving cars. \\nWith the growing availability of data and computing power, ML is becoming an essential tool for',\n",
       "  'metadata': {'title': '',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf',\n",
       "   'creationdate': '2025-10-18T18:28:51+00:00',\n",
       "   'keywords': '',\n",
       "   'producer': '',\n",
       "   'subject': '',\n",
       "   'format': 'PDF 1.3',\n",
       "   'doc_index': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'trapped': '',\n",
       "   'modDate': '',\n",
       "   'moddate': '',\n",
       "   'author': '',\n",
       "   'total_pages': 1,\n",
       "   'creator': '',\n",
       "   'page': 0,\n",
       "   'creationDate': 'D:20251018182851Z',\n",
       "   'source_file': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf',\n",
       "   'content_length': 998,\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf'},\n",
       "  'similarity_score': 0.017642498016357422,\n",
       "  'distance': 0.9823575019836426,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_06fa3382_0',\n",
       "  'content': 'Introduction to Machine Learning\\nMachine Learning (ML) is a subset of Artificial Intelligence that enables computers to learn patterns\\nand make decisions without being explicitly programmed. \\nInstead of following fixed rules, ML systems analyze historical data to find trends and relationships,\\nallowing them to make predictions or classifications on new data.\\nCommon types of machine learning include supervised learning, unsupervised learning, and\\nreinforcement learning. \\nSupervised learning relies on labeled datasets to train predictive models, while unsupervised\\nlearning discovers hidden structures within unlabeled data. \\nReinforcement learning involves agents learning optimal behavior through trial and error interactions\\nwith their environment.\\nMachine learning has applications across industries, from fraud detection and recommendation\\nsystems to speech recognition and self-driving cars. \\nWith the growing availability of data and computing power, ML is becoming an essential tool for',\n",
       "  'metadata': {'creationDate': 'D:20251018182851Z',\n",
       "   'creator': '',\n",
       "   'doc_index': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': '',\n",
       "   'format': 'PDF 1.3',\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'modDate': '',\n",
       "   'moddate': '',\n",
       "   'trapped': '',\n",
       "   'title': '',\n",
       "   'total_pages': 1,\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf',\n",
       "   'creationdate': '2025-10-18T18:28:51+00:00',\n",
       "   'source_file': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf',\n",
       "   'keywords': '',\n",
       "   'content_length': 998,\n",
       "   'page': 0,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\sample_machine_learning (1).pdf'},\n",
       "  'similarity_score': 0.017642498016357422,\n",
       "  'distance': 0.9823575019836426,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296d57c",
   "metadata": {},
   "source": [
    "### Integration VectorDB Context pipeline with LLM output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "878f2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG Pipeline with GROQ\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(api_key=groq_api_key, model_name=\"openai/gpt-oss-20b\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "\n",
    "### Simple RAG function: retrieve context + genrate response\n",
    "\n",
    "def rag_simple(query,retriever,llm, top_k=5):\n",
    "\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    print(\"Context\",context)\n",
    "    if not context:\n",
    "        return \"No relevant context found.\"\n",
    "\n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b678e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is ML?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n",
      "Context systems to speech recognition and self-driving cars. \n",
      "With the growing availability of data and computing power, ML is becoming an essential tool for\n",
      "solving complex problems in both academia and industry.\n",
      "\n",
      "systems to speech recognition and self-driving cars. \n",
      "With the growing availability of data and computing power, ML is becoming an essential tool for\n",
      "solving complex problems in both academia and industry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ML** stands for **Machine Learning**—a branch of artificial intelligence that uses data and computational power to train models that can learn patterns and make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is ML?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a96d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'ML'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"ML\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1451a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention is all you need'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 99.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: No relevant context found.\n",
      "Summary: The response indicates that no relevant context was found. Consequently, no additional information can be provided.\n",
      "History: {'question': 'what is attention is all you need', 'answer': 'No relevant context found.', 'sources': [], 'summary': 'The response indicates that no relevant context was found. Consequently, no additional information can be provided.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
